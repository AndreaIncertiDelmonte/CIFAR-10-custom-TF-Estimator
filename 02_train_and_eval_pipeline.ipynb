{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Training and evaluation pipeline\n",
    "Autor: **Andrea Incerti Delmonte**\n",
    "\n",
    "Email: ** andrea.incertidelmonte@gmail.com**\n",
    "\n",
    "This training pipeline is based on TF Estimator https://www.tensorflow.org/get_started/custom_estimators\n",
    "1. Hyperparameters and estimators configurations \n",
    "2. Load dataset metadata\n",
    "3. Input pipeline definition\n",
    "4. CNN network definition\n",
    "5. Custom TF Estimator\n",
    "6. Model training and evaluation\n",
    "7. Model evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_RECORDS_BASE_PATH = \"./cifar-10_dataset/tf_records/\"\n",
    "TRAIN_TFRECORDS = TF_RECORDS_BASE_PATH + \"train.tfrecords\"\n",
    "EVAL_TFRECORDS = TF_RECORDS_BASE_PATH + \"eval.tfrecords\"\n",
    "TEST_TFRECORDS = TF_RECORDS_BASE_PATH + \"test.tfrecords\"\n",
    "METADATA_PATH = \"./cifar-10_dataset/cifar-10-batches-py/batches.meta\"\n",
    "\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "IMG_CHANNELS = 3\n",
    "CLASS_NUMBER = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Hyperparameters and estimators configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS():\n",
    "  pass \n",
    "\n",
    "FLAGS.lenet_model_name = \"lenet\"\n",
    "FLAGS.deep_lenet_model_name = \"deep-lenet\"\n",
    "\n",
    "###############################\n",
    "#                             #\n",
    "# Choose which model to train #\n",
    "#                             #\n",
    "###############################\n",
    "\n",
    "FLAGS.model_name = FLAGS.deep_lenet_model_name\n",
    "\n",
    "FLAGS.batch_size = 128\n",
    "FLAGS.learning_rate = 0.001\n",
    "FLAGS.max_steps = 60000\n",
    "FLAGS.eval_steps = 1000\n",
    "FLAGS.throttle_secs = 2*60\n",
    "#FLAGS.tf_random_seed = 19861102\n",
    "FLAGS.save_checkpoints_steps = 2000\n",
    "FLAGS.use_checkpoint = False\n",
    "FLAGS.exported_model_name = \"exported-{}\".format(FLAGS.model_name)\n",
    "FLAGS.keep_checkpoint_max = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Cifar10 metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_f = open(METADATA_PATH, 'rb')\n",
    "metadata_dict = cPickle.load(metadata_f)\n",
    "labels_LUT = metadata_dict['label_names']\n",
    "print(labels_LUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TFRecords processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 From serialized record to image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_parser(record):\n",
    "    features = tf.parse_single_example(\n",
    "        record,\n",
    "        features={\n",
    "          'image': tf.FixedLenFeature([], tf.string),\n",
    "          'label': tf.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "    )\n",
    "    image = tf.decode_raw(features['image'], tf.uint8)\n",
    "    image = tf.reshape(image, shape=(IMG_CHANNELS, IMG_HEIGHT, IMG_WIDTH))\n",
    "    image = tf.transpose(image, [1, 2, 0])\n",
    "    \n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def augment(image, label):\n",
    "    \n",
    "    # Apply random crop\n",
    "    image = tf.image.resize_image_with_crop_or_pad(\n",
    "        image, IMG_HEIGHT + 4, IMG_WIDTH + 4)\n",
    "    image = tf.random_crop(image, [IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])\n",
    "    \n",
    "    # Apply vertical random split\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def normalize(image, label):\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training batch preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_input_fn(train_tfrecords, batch_size=1):\n",
    "    def _train_input_fn():\n",
    "        train_dataset = tf.data.TFRecordDataset(train_tfrecords)\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=(batch_size*2 + 1))\n",
    "        train_dataset = train_dataset.map(record_parser)\n",
    "        train_dataset = train_dataset.map(augment)\n",
    "        train_dataset = train_dataset.map(normalize)        \n",
    "        train_dataset = train_dataset.repeat()\n",
    "        train_dataset = train_dataset.batch(batch_size)\n",
    "        train_dataset = train_dataset.prefetch(batch_size*2)\n",
    "        train_iterator = train_dataset.make_one_shot_iterator()\n",
    "        images, labels = train_iterator.get_next()\n",
    "        features = {'images': images}\n",
    "        \n",
    "        return features, labels\n",
    "    return _train_input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Test training batch preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_train_input_fn = generate_train_input_fn(TRAIN_TFRECORDS, FLAGS.batch_size)\n",
    "images_tensor, labels_tensor = generated_train_input_fn()\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    images_batch_dict, labels_batch = sess.run([images_tensor, labels_tensor])\n",
    "    images_batch = images_batch_dict[\"images\"]\n",
    "    print(\"Train images batch shape {}\".format(images_batch.shape))\n",
    "    print(\"Train labels batch shape {}\".format(labels_batch.shape))\n",
    "\n",
    "    print(\"Image batch type {}\".format(type(images_batch[0][0][0][0])))\n",
    "    print(\"Label batch type {}\".format(type(labels_batch[0])))\n",
    "\n",
    "for i in range(min([2, images_batch.shape[0]])):\n",
    "    \n",
    "    image = images_batch[i]\n",
    "\n",
    "    # Check image shape\n",
    "    assert image.shape == (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "\n",
    "    # Plot the image\n",
    "    plt.title(\"Train image label {} class {}\".format(labels_batch[i], labels_LUT[labels_batch[i]]))\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluation batch preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval_input_fn(eval_tfrecords, batch_size=1):\n",
    "    def _eval_input_fn():\n",
    "        eval_dataset = tf.data.TFRecordDataset(eval_tfrecords)\n",
    "        eval_dataset = eval_dataset.map(record_parser)\n",
    "        eval_dataset = eval_dataset.map(normalize)\n",
    "        eval_dataset = eval_dataset.repeat()\n",
    "        eval_dataset = eval_dataset.batch(batch_size)\n",
    "        eval_dataset = eval_dataset.prefetch(batch_size*2)\n",
    "        eval_iterator = eval_dataset.make_one_shot_iterator()\n",
    "        images, labels = eval_iterator.get_next()\n",
    "        features = {'images': images}\n",
    "        \n",
    "        return features, labels\n",
    "    return _eval_input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Test evaluation batch preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_eval_input_fn = generate_eval_input_fn(EVAL_TFRECORDS, FLAGS.batch_size)\n",
    "images_tensor, labels_tensor = generated_eval_input_fn()\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    images_batch_dict, labels_batch = sess.run([images_tensor, labels_tensor])\n",
    "    images_batch = images_batch_dict[\"images\"]\n",
    "    print(\"Eval images batch shape {}\".format(images_batch.shape))\n",
    "    print(\"Eval labels batch shape {}\".format(labels_batch.shape))\n",
    "\n",
    "    print(\"Eval batch type {}\".format(type(images_batch[0][0][0][0])))\n",
    "    print(\"Eval batch type {}\".format(type(labels_batch[0])))\n",
    "\n",
    "for i in range(min([2, images_batch.shape[0]])):\n",
    "    \n",
    "    image = images_batch[i]\n",
    "\n",
    "    # Check image shape\n",
    "    assert image.shape == (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "\n",
    "    # Plot the image\n",
    "    plt.title(\"Eval image label {} class {}\".format(labels_batch[i], labels_LUT[labels_batch[i]]))\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CNN network definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_variable(shape, mu=0, sigma=0.1):\n",
    "    \"\"\"\n",
    "    weight_variable generates a weight variable of a given shape.\n",
    "    \"\"\"\n",
    "    initial = tf.truncated_normal(shape, mean=mu, stddev=sigma)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def init_bias_variable(shape, value=0.1):\n",
    "    \"\"\"\n",
    "    bias_variable generates a bias variable of a given shape.\n",
    "    \"\"\"\n",
    "    initial = tf.constant(value=value, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def LeNet(x):    \n",
    "    \n",
    "    # Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.\n",
    "    with tf.name_scope(\"conv1\"):\n",
    "        W_conv1 = init_weight_variable([5, 5, 3, 6])\n",
    "        b_conv1 = init_bias_variable([6])\n",
    "        c_conv1 = tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='VALID') + b_conv1\n",
    "        h_conv1 = tf.nn.relu(c_conv1)\n",
    "        print(\"h_conv1.shape {}\".format(h_conv1.shape))\n",
    "  \n",
    "    # Layer 1: Max Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    with tf.name_scope(\"pool1\"):\n",
    "        h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "        print(\"h_pool1.shape {}\".format(h_pool1.shape))\n",
    "\n",
    "    \n",
    "    # Layer 2: Convolutional. Input = 14x14x6. Output = 10x10x16.\n",
    "    with tf.name_scope(\"conv2\"):\n",
    "        W_conv2 = init_weight_variable([5, 5, 6, 16])\n",
    "        b_conv2 = init_bias_variable([16])\n",
    "        c_conv2 = tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='VALID') + b_conv2\n",
    "        h_conv2 = tf.nn.relu(c_conv2)\n",
    "        print(\"h_conv2.shape {}\".format(h_conv2.shape))\n",
    "    \n",
    "    # Layer 2: Max Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    with tf.name_scope(\"pool2\"):\n",
    "        h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "        print(\"h_pool2.shape {}\".format(h_pool2.shape))\n",
    "    \n",
    "    # Flatten. Input = 5x5x16. Output = 400. \n",
    "    with tf.name_scope(\"fc0\"):\n",
    "        fc0 = tf.layers.flatten(h_pool2)\n",
    "        print(\"flatten.shape {}\".format(fc0.shape))\n",
    "    \n",
    "    # Layer 3: Fully connected. Input = 400. Output = 120. \n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        W_fc1 = init_weight_variable([400, 120])\n",
    "        b_fc1 = init_bias_variable([120])\n",
    "        mm_fc1 = tf.matmul(fc0, W_fc1) + b_fc1\n",
    "        h_fc1 = tf.nn.relu(mm_fc1)\n",
    "        print(\"h_fc1.shape {}\".format(h_fc1.shape))\n",
    "\n",
    "    # Layer 4: Fully connected. Input = 120. Output = 84. \n",
    "    with tf.name_scope(\"fc2\"):\n",
    "        W_fc2 = init_weight_variable([120, 84])\n",
    "        b_fc2 = init_bias_variable([84])\n",
    "        mm_fc2 = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "        h_fc2 = tf.nn.relu(mm_fc2)\n",
    "        print(\"h_fc2.shape {}\".format(h_fc2.shape))\n",
    "    \n",
    "    # Layer 5: Fully connected. Input = 84. Output = 10. \n",
    "    with tf.name_scope(\"fc3\"):\n",
    "        W_fc3 = init_weight_variable([84, 10])\n",
    "        b_fc3 = init_bias_variable([10])\n",
    "        logits = tf.matmul(h_fc2, W_fc3) + b_fc3\n",
    "        print(\"h_fc3.shape {}\".format(logits.shape))\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Deep LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepLeNet(images):\n",
    "             \n",
    "    # Layer 1: Convolutional. Input = 32x32x3. Output = 32x32x64.\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=images, filters=64, kernel_size=[5, 5], padding='SAME',\n",
    "      activation=tf.nn.relu, name='Conv1')\n",
    "    print(\"conv1.shape {}\".format(conv1.shape))\n",
    "    \n",
    "    # Layer 1: Max Pooling. Input = 32x32x64. Output = 15x15x64.\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "      inputs=conv1, pool_size=[3, 3], strides=2, name='MaxPool1')\n",
    "    norm1 = tf.nn.lrn(\n",
    "      pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='Norm1')\n",
    "    print(\"pool1.shape {}\".format(pool1.shape))\n",
    "\n",
    "    # Layer 2: Convolutional. Input = 15x15x64. Output = 15x15x64.                                                                                                                \n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=norm1, filters=64, kernel_size=[5, 5], padding='SAME',\n",
    "      activation=tf.nn.relu, name='Conv2')\n",
    "    print(\"conv2.shape {}\".format(conv2.shape))\n",
    "    norm2 = tf.nn.lrn(\n",
    "      conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='Norm2')\n",
    "    \n",
    "    # Layer 2: Max Pooling. Input = 15x15x64. Output = 7x7x64.\n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "      inputs=norm2, pool_size=[3, 3], strides=2, name='Pool2')\n",
    "    print(\"pool2.shape {}\".format(pool2.shape))\n",
    "\n",
    "    # Flatten. Input = 7x7x64. Output = 3136.                                                                                                                         \n",
    "    shape = pool2.get_shape()\n",
    "    pool2_ = tf.reshape(pool2, [-1, shape[1]*shape[2]*shape[3]])\n",
    "    print(\"flatten.shape {}\".format(pool2_.shape))\n",
    "\n",
    "    # Layer 3: Fully connected. Input = 3136. Output = 384.                                                                                                                \n",
    "    dense1 = tf.layers.dense(\n",
    "      inputs=pool2_, units=384, activation=tf.nn.relu, name='FC1')\n",
    "    print(\"dense1.shape {}\".format(dense1.shape))\n",
    "    \n",
    "    # Layer 4: Fully connected. Input = 384. Output = 192.                                                                                                       \n",
    "    dense2 = tf.layers.dense(\n",
    "      inputs=dense1, units=192, activation=tf.nn.relu, name='FC2')\n",
    "    print(\"dense2.shape {}\".format(dense2.shape))\n",
    "\n",
    "    # Layer 5: Fully connected. Input = 192. Output = 10.                                                                                                       \n",
    "    logits = tf.layers.dense(\n",
    "      inputs=dense2, units=CLASS_NUMBER, activation=tf.nn.relu, name='Logits')\n",
    "    print(\"logits.shape {}\".format(logits.shape))\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom TF Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Define features columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns():\n",
    "  feature_columns = {\n",
    "    'images': tf.feature_column.numeric_column('images', (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),\n",
    "  }\n",
    "  return feature_columns\n",
    "\n",
    "feature_columns = get_feature_columns()\n",
    "print(\"Feature columns: {}\".format(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 TF Estimator model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    # Create the input layers from the features                                                                                               \n",
    "    feature_columns = list(get_feature_columns().values())\n",
    "\n",
    "    images = tf.feature_column.input_layer(features=features, feature_columns=feature_columns)\n",
    "    images = tf.reshape(images, shape=(-1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    \n",
    "    if FLAGS.model_name == FLAGS.lenet_model_name:\n",
    "        logits = LeNet(images)\n",
    "    else:\n",
    "        logits = DeepLeNet(images)\n",
    "    \n",
    "    predictions = {\n",
    "        'class_ids': tf.argmax(input=logits, axis=1, name='classes'),\n",
    "        'probabilities': tf.nn.softmax(logits, name='softmax_tensor'),\n",
    "        'logits': logits\n",
    "    }\n",
    "\n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        \n",
    "        export_outputs = {\n",
    "            'predictions': tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions, export_outputs=export_outputs)\n",
    "    \n",
    "    # For train and eval\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    # Compute the loss for both training and evaluation\n",
    "    one_hot_labels = tf.one_hot(labels, CLASS_NUMBER)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot_labels, logits=logits)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    # Display cross_entropy metric into TensorBoard\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "    \n",
    "    # Compute the accuracy.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions['class_ids'], name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    # Display accuracy metric into TensorBoard\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    # If evaluation mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)    \n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # From tutorial\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode, \n",
    "        loss=loss, \n",
    "        train_op=train_op, \n",
    "        predictions=predictions, \n",
    "        eval_metric_ops=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Serving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_serving(image):\n",
    "    \n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def serving_input_fn():\n",
    "    receiver_tensor = {\n",
    "        'images': tf.placeholder(shape=[None, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS], dtype=tf.float32)\n",
    "    }\n",
    "    features = {\n",
    "        'images': tf.map_fn(preprocess_for_serving, receiver_tensor['images'])\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 TF Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Estimator configurations\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "    #tf_random_seed=FLAGS.tf_random_seed,\n",
    "    model_dir=\"./trained_models/{}\".format(FLAGS.model_name),\n",
    "    keep_checkpoint_max = FLAGS.keep_checkpoint_max\n",
    ")\n",
    "\n",
    "# Build the Estimator\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=model_fn, \n",
    "    params={},\n",
    "    config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Train and eval configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=generate_train_input_fn(TRAIN_TFRECORDS, batch_size=FLAGS.batch_size),\n",
    "    max_steps=FLAGS.max_steps\n",
    ")\n",
    "\n",
    "# Used to export the model\n",
    "exporter = tf.estimator.FinalExporter(\n",
    "    name=FLAGS.exported_model_name,\n",
    "    serving_input_receiver_fn=serving_input_fn,\n",
    "    assets_extra=None,\n",
    "    as_text=False,\n",
    ")\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=generate_eval_input_fn(EVAL_TFRECORDS, batch_size=FLAGS.batch_size),\n",
    "    steps=FLAGS.eval_steps, \n",
    "    throttle_secs=FLAGS.throttle_secs,\n",
    "    exporters=exporter    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier = tf.estimator.Estimator(model_fn=model_fn, config=run_config)\n",
    "test_classifier.evaluate(input_fn=generate_eval_input_fn(TEST_TFRECORDS, FLAGS.batch_size), steps=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_27",
   "language": "python",
   "name": "tensorflow_27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
